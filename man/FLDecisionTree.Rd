% Generated by roxygen2 (4.0.2): do not edit by hand
\name{FLDecisionTree}
\alias{FLDecisionTree}
\title{Decision Tree}
\usage{
FLDecisionTree(table, primary_key, response, min_obs_for_parent, max_level,
  purity_threshold, exclude = c(), class_spec = list(), where_clause = "",
  note = "From RWrapper For DBLytix")
}
\arguments{
\item{table}{an object of class \code{FLTable}}

\item{primary_key}{name of primary key column of the table mapped to \code{table}}

\item{response}{name of the dependent variable column of the table mapped to
\code{table}}

\item{min_obs_for_parent}{minimum number of observations for a given node to
be a parent node. It is used for early termination of some iterations once a
given node has less than \code{min_obs_for_parent} number of observations.
Note that \code{min_obs_for_parent} > 1}

\item{max_level}{positive integer specifying the maximum tree level this
tree can grow to}

\item{purity_threshold}{threshold used for early termination of some iteration
once a certain percentage (specified by this threshold) of records has the
same class value.
Note that  1 > \code{purity_threshold} > 0}

\item{exclude}{vector of names of the columns which are to be excluded}

\item{class_spec}{list that identifies the value of the categorical variable
which is to be used as reference when converting to dummy binary variables}

\item{where_clause}{condition to filter out data from the table}

\item{note}{note}
}
\value{
\code{FLDecisionTree} returns an object of class \code{FLDecisionTree}.
The components of this class mentioned below can be pulled in R using the
generic \code{FLFetch}.
\item{node_info}{a \code{data.frame} which stores all nodes of the decision tree built, including leave
nodes and decision nodes (also known as splitting nodes).}
\item{classification}{a \code{data.frame} which stores observations' classifications}
}
\description{
Performs decision tree analysis of the data.
}
\details{
Decision tree is one of the most widely used classification algorithms.
Input data includes a set of Y (dependent variable, also known as target
variable or class) values and multiple sets of X (independent variables,
also known as predictor variables) values.
The input data table, also known as a training data set, is a wide table.
All values in such a table are numeric. The class variable is categorical,
with two or more categories.
Depending on different independent variable types, the function \code{FLDecisionTree}
will use a different algorithm to build the decision tree. If the independent
variables have continuous values, a method that is based on the CART algorithm
is used. During this process, independent variables can be reused if needed.
If the independent variables have binary values, a method based on the ID3 algorithm
is used. Variables are not reused in this process. In both cases, a binary
decision tree is constructed by splitting a node into two child nodes repeatedly.
}
\examples{
\dontrun{
connection <- odbcConnect("Gandalf")
db_name    <- FL_R_WRAP
table_name <- "tblAutoMpg"
# Create FLTable object
table      <-  FLTable(connection, db_name, table_name)
# Perform Decision Tree Analysis
result     <- FLDecisionTree(table,
                             primary_key = "ObsID",
                             response = "Weight",
                             min_obs_for_parent = 10,
                             max_level = 5,
                             purity_threshold = 0.95,
                             exclude = c("CarNum", "CarNumber"),
                             class_spec = list(CarName = "BMW"))
# Fetch reults in R
decisionTreeResult <- FLFetch(result)
}
}

